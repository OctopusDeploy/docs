---
layout: src/layouts/Default.astro
pubDate: 2023-01-01
modDate: 2024-05-22
title: Local File Storage
description: Guidelines and recommendations for configuring local file storage with Octopus Deploy.
navOrder: 10
hideInThisSection: true
---
import FileStorageHA from 'src/shared-content/installation/file-storage-ha.include.md';

When you opt to store the binary files on local network storage, there are a few items to keep in mind:

- When Octopus is hosted on Windows it can be a mapped network drive e.g. `X:\` or a UNC path to a file share e.g. `\\server\share`.
- When Octopus is hosted as a container it must be as a volume.
- The service account that Octopus runs needs **full control** over the directory.
- Drives are mapped per-user, so you should map the drive using the same service account that Octopus is running under.

Most commercial network storage solutions will work perfectly well with Octopus Deploy.  They will use industry-standard mechanisms for data integrity, for example, RAID or ZFS, and have the capability to perform regular backups.  

## High Availability

<FileStorageHA />

## Disaster Recovery

For disaster recovery scenarios, [we recommend leveraging a hot/cold configuration](https://octopus.com/whitepapers/best-practice-for-self-hosted-octopus-deploy-ha-dr).  The file system should asynchronously copy files to a secondary data center.  When a disaster occurs, create the nodes in the secondary data center and configure them to use that secondary file storage location.

There are many robust syncing solutions, but even simple tools like rsync or robocopy will work.  As long as they periodically run, that is all that matters.

One common approach we've seen is leveraging a distributed file system (DFS) such as [Microsoft DFS](https://en.wikipedia.org/wiki/Distributed_File_System_(Microsoft)).  DFS will work with Octopus Deploy, but it must be configured in a specific way.  Unless you configure it properly you'll encounter a [split-brain](https://en.wikipedia.org/wiki/Split-brain_(computing)) scenario.

## DFS

:::div{.warning}
**DFS in the standard configuration (i.e., accessed through a DFS Namespace Root) is _not_ suitable for use as a shared file store with Octopus Deploy.**

Operating Octopus Deploy with the non-recommended DFS configuration will likely result in intermittent and potentially significant issues.
:::

Below are recommendations and more details on:

- [High Availability](#high-availability)
- [Disaster Recovery](#disaster-recovery)
- [DFS](#dfs)
  - [Configuring DFS with a Single Octopus Server](#configuring-dfs-with-a-single-octopus-server)
  - [Configuring DFS with a Multi-Node Octopus Server cluster (Octopus HA)](#configuring-dfs-with-a-multi-node-octopus-server-cluster-octopus-ha)
  - [DFS for Redundancy (Disaster Recovery)](#dfs-for-redundancy-disaster-recovery)

### Configuring DFS with a Single Octopus Server

For a single-node Octopus Server using DFS for file storage, the node must be **configured to use a specific DFS Replica and not the DFS Namespace Root**. Despite no contention between nodes in the single-node configuration, there is still the DFS location transparency, which will cause unpredictable behavior when the node is directed to a different replica.

In the diagram, the single node is configured to use the replica `\\SVR_ONE\public` as the DFS file share and not the namespace root (`\\Contoso\public`). 

:::figure
![A single Octopus Deploy node with DFS shared storage](/docs/getting-started/best-practices/images/single-node-od-with-dfs.png)
:::

### Configuring DFS with a Multi-Node Octopus Server cluster (Octopus HA)

For a multi-node Octopus cluster using DFS for file storage, it is imperative that **_all_ nodes in the cluster are configured to use the same DFS Replica and not the DFS Namespace Root**. Both using the namespace root or using different replicas for different Octopus nodes will cause unpredictable behavior.

In the diagram below each node in the cluster is configured to use the same replica (`\\SVR_ONE\public`) as the DFS file share and not the namespace root (`\\Contoso\public`). 

:::figure
![A multi-node (HA) Octopus Cluster with DFS shared storage](/docs/getting-started/best-practices/images/multi-node-od-with-dfs.png)
:::

### DFS for Redundancy (Disaster Recovery)

DFS can still be used for redundancy and disaster recovery, as usual.

If the replica that Octopus is configured to use becomes unavailable, simply changing the configuration to another replica in the DFS Namespace with the same target folders is sufficient to restore service.

Octopus does not need to be restarted in this scenario. Customers can either do this manually or can automate it.

In the simplified diagram below, when an outage at DFS Replica `\\SVR_ONE\Public` occurs, by re-configuring each Octopus node to use a different replica (ensuring all nodes are re-configured to the same replica), customers can still take advantage of the redundancy within DFS.

![Using DFS for redundancy with Octopus Deploy](/docs/getting-started/best-practices/images/dfs-for-redundancy.png)